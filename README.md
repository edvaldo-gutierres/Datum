# Projeto Datum 

Ol√° Datum, espero que esteje tudo bem com voc√™s.

Eu sou Edvaldo Gutierres - [LinkedIn](https://www.linkedin.com/in/edvaldo-gutierres-6b4a5768/)

üìä Este reposit√≥rio cont√©m todos os notebooks e scripts utilizados no meu teste t√©cnino  para a vaga de Desenvolvedor Python (Engenheiro de Dados).

**Objetivo**: O projeto foca na an√°lise de dados de vendas do dataset Olist E-commerce dispon√≠vel no Kaggle, seguindo as melhores pr√°ticas de ETL (Extract, Transform, Load) e prepara√ß√£o de dados para analise de dados.

![!\[worflow/olist.png)](Worflows/olist.png)

## üìÅ Estrutura do Projeto

O projeto est√° organizado em v√°rias pastas correspondentes √†s camadas de processamento de dados: `Ingestion`, `Bronze`, `Silver`, e `Gold`, al√©m de uma pasta `Data Mining` para explora√ß√£o de dados e `Worflow` para detalhar as job criadas.

---
### üîó Ingestion
Aqui realizamos a conex√£o com a API do Kaggle para baixar o dataset Olist E-commerce. Os dados s√£o inicialmente salvos na camada `Raw` no DBFS do Databricks, com possibilidade de armazenamento em solu√ß√µes de nuvem como Azure Data Lake Storage Gen2.

---
### üîÑ Bronze
Ap√≥s a ingest√£o dos dados na camada `Raw`, processamos e adicionamos a coluna `row_ingestion_timestamp`. Os dados s√£o ent√£o armazenados em formato Parquet na camada `Bronze`. Importante: em um ambiente de produ√ß√£o, considerar√≠amos estrat√©gias de carga avan√ßadas e o particionamento de dados para otimizar a gest√£o de dados.

---
### üîç Silver
Nesta etapa, aplicamos t√©cnicas de qualidade de dados, como remo√ß√£o de duplicatas e tratamento de nulos, e armazenamos os dados limpos na camada `Silver` em formato Parquet ou Delta, conforme a fun√ß√£o escolhida. Tamb√©m inclu√≠mos uma tabela intermedi√°ria `inter_sales` para auxiliar na modelagem dos dados.

---
### üèÜ Gold
Para o desenvolvimento da camada `Gold`, foi definido perguntas simples com foco em responder quest√µes de neg√≥cio como a quantidade de pedidos por diferentes dimens√µes. 
Sendo elas:
    - Quantidade de Pedidos por:
        - Status
        - Tempo
        - Cliente
        - Produto

 Preparamos os dados para serem consumidos por ferramentas de visualiza√ß√£o, seguindo as boas pr√°ticas de modelagem para Power BI (schema em estrela). Foi aplicados conceitos de modelagem starschema propostos por Ralph Kimball.

---
### ‚õèÔ∏è Mining
Realizamos uma explora√ß√£o de dados para entender melhor os padr√µes e validar as transforma√ß√µes. Criamos visualiza√ß√µes e relat√≥rios para comunicar insights, como a tend√™ncia de pedidos ao longo do tempo e flutua√ß√µes sazonais.

---
## üöÄ Workflows
Todas as etapas de processamento de dados foram implementadas e agendadas como jobs no Databricks, assegurando a automa√ß√£o e a execu√ß√£o eficiente do pipeline de dados. As jobs est√£o configuradas para garantir que a ingest√£o, o processamento e a modelagem dos dados sejam realizados sequencialmente, com a garantia de integridade e atualiza√ß√£o dos dados.

As jobs do Databricks tamb√©m facilitam a colabora√ß√£o entre membros da equipe e a escalabilidade do projeto, permitindo ajustes e expans√µes conforme necess√°rio. A configura√ß√£o detalhada e os logs de execu√ß√£o das jobs podem ser encontrados na pasta.

---
## üôè Agradecimentos
---

Agrade√ßo pela oportunidade, qualquer d√∫vida estou sempre a disposi√ß√£o.
