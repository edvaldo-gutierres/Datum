# Projeto Datum 

OlÃ¡ Datum, espero que esteje tudo bem com vocÃªs.

Eu sou Edvaldo Gutierres - [LinkedIn](https://www.linkedin.com/in/edvaldo-gutierres-6b4a5768/)

ğŸ“Š Este repositÃ³rio contÃ©m todos os notebooks e scripts utilizados no meu teste tÃ©cnino  para a vaga de Desenvolvedor Python (Engenheiro de Dados).

**Objetivo**: O projeto foca na anÃ¡lise de dados de vendas do dataset Olist E-commerce disponÃ­vel no Kaggle, seguindo as melhores prÃ¡ticas de ETL (Extract, Transform, Load) e preparaÃ§Ã£o de dados para analise de dados.

![alt text](olist.png)

## ğŸ“ Estrutura do Projeto

O projeto estÃ¡ organizado em vÃ¡rias pastas correspondentes Ã s camadas de processamento de dados: `Ingestion`, `Bronze`, `Silver`, e `Gold`, alÃ©m de uma pasta `Data Mining` para exploraÃ§Ã£o de dados e `Worflow` para detalhar as job criadas.

---
### ğŸ”— Ingestion
Aqui realizamos a conexÃ£o com a API do Kaggle para baixar o dataset Olist E-commerce. Os dados sÃ£o inicialmente salvos na camada `Raw` no DBFS do Databricks, com possibilidade de armazenamento em soluÃ§Ãµes de nuvem como Azure Data Lake Storage Gen2.

---
### ğŸ”„ Bronze
ApÃ³s a ingestÃ£o dos dados na camada `Raw`, processamos e adicionamos a coluna `row_ingestion_timestamp`. Os dados sÃ£o entÃ£o armazenados em formato Parquet na camada `Bronze`. Importante: em um ambiente de produÃ§Ã£o, considerarÃ­amos estratÃ©gias de carga avanÃ§adas e o particionamento de dados para otimizar a gestÃ£o de dados.

---
### ğŸ” Silver
Nesta etapa, aplicamos tÃ©cnicas de qualidade de dados, como remoÃ§Ã£o de duplicatas e tratamento de nulos, e armazenamos os dados limpos na camada `Silver` em formato Parquet ou Delta, conforme a funÃ§Ã£o escolhida. TambÃ©m incluÃ­mos uma tabela intermediÃ¡ria `inter_sales` para auxiliar na modelagem dos dados.

---
### ğŸ† Gold
Para o desenvolvimento da camada `Gold`, foi definido perguntas simples com foco em responder questÃµes de negÃ³cio como a quantidade de pedidos por diferentes dimensÃµes. 
Sendo elas:
    - Quantidade de Pedidos por:
        - Status
        - Tempo
        - Cliente
        - Produto

 Preparamos os dados para serem consumidos por ferramentas de visualizaÃ§Ã£o, seguindo as boas prÃ¡ticas de modelagem para Power BI (schema em estrela). Foi aplicados conceitos de modelagem starschema propostos por Ralph Kimball.

---
### â›ï¸ Mining
Realizamos uma exploraÃ§Ã£o de dados para entender melhor os padrÃµes e validar as transformaÃ§Ãµes. Criamos visualizaÃ§Ãµes e relatÃ³rios para comunicar insights, como a tendÃªncia de pedidos ao longo do tempo e flutuaÃ§Ãµes sazonais.

---
## ğŸš€ Workflows
Todas as etapas de processamento de dados foram implementadas e agendadas como jobs no Databricks, assegurando a automaÃ§Ã£o e a execuÃ§Ã£o eficiente do pipeline de dados. As jobs estÃ£o configuradas para garantir que a ingestÃ£o, o processamento e a modelagem dos dados sejam realizados sequencialmente, com a garantia de integridade e atualizaÃ§Ã£o dos dados.

As jobs do Databricks tambÃ©m facilitam a colaboraÃ§Ã£o entre membros da equipe e a escalabilidade do projeto, permitindo ajustes e expansÃµes conforme necessÃ¡rio. A configuraÃ§Ã£o detalhada e os logs de execuÃ§Ã£o das jobs podem ser encontrados na pasta.

---
## ğŸ™ Agradecimentos
---

AgradeÃ§o pela oportunidade, qualquer dÃºvida estou sempre a disposiÃ§Ã£o.
