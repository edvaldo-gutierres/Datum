{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f1f8bfd-a674-4182-bffe-6e76510a6e3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define os parâmetros da Job\n",
    "dbutils.widgets.text('p_path','')\n",
    "dbutils.widgets.text('p_layer','')\n",
    "dbutils.widgets.text('p_file_name_bronze','')\n",
    "\n",
    "dbutils.widgets.text('p_file_name_silver','')\n",
    "dbutils.widgets.text('p_mode_write_silver','')\n",
    "dbutils.widgets.text('p_modo_write_delta','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "854f2d19-8b47-49bd-92bb-72b8ee5243ca",
     "showTitle": true,
     "title": "Importa Bibliotecas"
    }
   },
   "outputs": [],
   "source": [
    "# Importando Bibliotecas\n",
    "from pyspark.sql import DataFrame, Row\n",
    "from pyspark.sql.functions import current_timestamp, col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, NumericType, DateType, BooleanType, TimestampType\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68dbf560-8e91-4519-8e2a-993becf07da2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Funções para manipular arquivos parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca866d63-0d53-4543-8aa5-7abc6745ced5",
     "showTitle": true,
     "title": "Cria função para ler arquivos parquet"
    }
   },
   "outputs": [],
   "source": [
    "def ler_parquet(file_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Lê um arquivo Parquet no Spark.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Caminho do arquivo Parquet.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame resultante da leitura do arquivo Parquet.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Lê o arquivo Parquet\n",
    "        df = spark.read.parquet(file_path)\n",
    "        print(\"Leitura do arquivo Parquet bem-sucedida.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler o arquivo Parquet: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f1605cc-9cad-4182-9ad2-0723ed3e1eb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def gravar_parquet(df :DataFrame, destination_path :str, mode_write_bronze :str) -> None:\n",
    "    \"\"\"\n",
    "    Grava os dados de um DataFrame no formato Parquet.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame do Spark.\n",
    "        destination_path (str): Caminho de destino para salvar o arquivo Parquet.\n",
    "    \"\"\"\n",
    "    # Grava o DataFrame no formato Parquet\n",
    "    df.write.mode(mode_write_bronze).parquet(destination_path)\n",
    "\n",
    "    print(\"Dados salvos com sucesso no formato Parquet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8659de7c-eada-4b4e-8cbb-e4cd7fe44e02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Função para tratamento de dados duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e612e72a-de4c-411b-b896-a92db30f1858",
     "showTitle": true,
     "title": "Cria função para tratamento de dados duplicados"
    }
   },
   "outputs": [],
   "source": [
    "def verificar_e_remover_duplicatas(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Verifica se há linhas duplicadas em um DataFrame do Spark e remove as duplicatas.\n",
    "\n",
    "    Args:\n",
    "    dataframe (DataFrame): DataFrame do Spark a ser processado.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame resultante após a remoção das linhas duplicadas, ou None se ocorrer um erro.\n",
    "    \"\"\"\n",
    "\n",
    "    # Verifica se o argumento é um DataFrame do Spark\n",
    "    if not isinstance(df, DataFrame):\n",
    "        print('Erro: O argumento não é um DataFrame do Spark.')\n",
    "\n",
    "    try:\n",
    "        # Verifica se há linhas duplicadas\n",
    "        num_duplicatas = df.count() - df.dropDuplicates().count()\n",
    "\n",
    "        # Se houver duplicatas, remove-as\n",
    "        if num_duplicatas > 0:\n",
    "            print('Existem linhas duplicadas. Removendo...')\n",
    "            df_no_duplicates = df.dropDuplicates()\n",
    "            return df_no_duplicates\n",
    "        else:\n",
    "            print('NÃO existem linhas duplicadas')\n",
    "            return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao verificar duplicatas: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34e17e1e-5b7a-4c1d-b566-8f7058be4c23",
     "showTitle": true,
     "title": "Teste da Função"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id_pais</th><th>nome_pais</th></tr></thead><tbody><tr><td>1</td><td>Brasil</td></tr><tr><td>2</td><td>Argentina</td></tr><tr><td>3</td><td>Chile</td></tr><tr><td>4</td><td>Peru</td></tr><tr><td>1</td><td>Brasil</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Brasil"
        ],
        [
         2,
         "Argentina"
        ],
        [
         3,
         "Chile"
        ],
        [
         4,
         "Peru"
        ],
        [
         1,
         "Brasil"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id_pais",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "nome_pais",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existem linhas duplicadas. Removendo...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id_pais</th><th>nome_pais</th></tr></thead><tbody><tr><td>1</td><td>Brasil</td></tr><tr><td>2</td><td>Argentina</td></tr><tr><td>3</td><td>Chile</td></tr><tr><td>4</td><td>Peru</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Brasil"
        ],
        [
         2,
         "Argentina"
        ],
        [
         3,
         "Chile"
        ],
        [
         4,
         "Peru"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id_pais",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "nome_pais",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define o esquema do DataFrame de exemplo\n",
    "schema = StructType([\n",
    "    StructField(\"id_pais\", IntegerType(), True),\n",
    "    StructField(\"nome_pais\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Dados do DataFrame de exemplo\n",
    "data = [\n",
    "    [1,'Brasil'],\n",
    "    [2,'Argentina'],\n",
    "    [3,'Chile'],\n",
    "    [4,'Peru'],\n",
    "    [1,'Brasil']\n",
    "]\n",
    "\n",
    "# Cria o DataFrame\n",
    "df_test_duplicates = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Exibe o DataFrame\n",
    "display(df_test_duplicates)\n",
    "\n",
    "# Chamar a função para tratamento de valores duplicatas no DataFrame\n",
    "df_trat_duplicates = verificar_e_remover_duplicatas(df_test_duplicates)\n",
    "\n",
    "display(df_trat_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecfc0055-66c3-47a1-ae05-aa216deee4e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Função para tratamento de dados com valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1407fdbc-7a8e-476d-a126-fe35364e8dd6",
     "showTitle": true,
     "title": "Cria função para tratamento de valores nulos"
    }
   },
   "outputs": [],
   "source": [
    "def verificar_e_tratar_valores_nulos(df :DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Verifica e trata os valores nulos em um DataFrame do Spark.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): DataFrame do Spark a ser processado.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame resultante após o tratamento dos valores nulos, ou None se ocorrer um erro.\n",
    "    Tipos de colunas e seu tratamento:\n",
    "      - Numérico: 0\n",
    "      - Não numérico: 'Não informado'\n",
    "      - Data: '1900-01-01'\n",
    "      - Data e hora: '1900-01-01 00:00:00'\n",
    "      - Booleano: False\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Conta a quantidade de valores nulos por coluna\n",
    "        valores_nulos_por_coluna = {}\n",
    "        for coluna in df.columns:\n",
    "            num_nulos_na_coluna = df.where(df[coluna].isNull()).count()\n",
    "            valores_nulos_por_coluna[coluna] = num_nulos_na_coluna\n",
    "\n",
    "        # Mapeia cada coluna para seu valor padrão correspondente\n",
    "        valores_padrao = {}\n",
    "        for coluna in df.columns:\n",
    "            tipo_coluna = df.schema[coluna].dataType\n",
    "\n",
    "            if isinstance(tipo_coluna, IntegerType):\n",
    "                valores_padrao[coluna] = 0\n",
    "            elif isinstance(tipo_coluna, StringType):\n",
    "                valores_padrao[coluna] = \"Não informado\"\n",
    "            elif isinstance(tipo_coluna, DateType):\n",
    "                valores_padrao[coluna] = \"1900-01-01\"\n",
    "            elif isinstance(tipo_coluna, TimestampType):\n",
    "                valores_padrao[coluna] = \"1900-01-01 00:00:00\"\n",
    "            elif isinstance(tipo_coluna, BooleanType):\n",
    "                valores_padrao[coluna] = False\n",
    "\n",
    "        # Preenche os valores nulos em todas as colunas com os valores padrão\n",
    "        df = df.fillna(valores_padrao)\n",
    "\n",
    "        print('Valores nulos tratados por coluna:')\n",
    "        for coluna, num_nulos_tratados in valores_nulos_por_coluna.items():\n",
    "            print(f'{coluna}: {num_nulos_tratados}')\n",
    "\n",
    "        print('Todos os valores nulos foram tratados.')\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Erro ao verificar e tratar valores nulos: {e}')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b84a7a83-04a0-42d7-9898-060bb0aa6320",
     "showTitle": true,
     "title": "Teste da Função"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>nome</th><th>idade</th><th>genero</th><th>ativo</th><th>data_nascimento</th><th>ultima_atualizacao</th></tr></thead><tbody><tr><td>1</td><td>João</td><td>30</td><td>Masculino</td><td>true</td><td>1992-05-15</td><td>2024-04-29T06:55:34.990153Z</td></tr><tr><td>2</td><td>Maria</td><td>null</td><td>Feminino</td><td>false</td><td>null</td><td>2024-04-29T06:55:34.990156Z</td></tr><tr><td>3</td><td>null</td><td>25</td><td>null</td><td>true</td><td>1997-10-20</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "João",
         30,
         "Masculino",
         true,
         "1992-05-15",
         "2024-04-29T06:55:34.990153Z"
        ],
        [
         2,
         "Maria",
         null,
         "Feminino",
         false,
         null,
         "2024-04-29T06:55:34.990156Z"
        ],
        [
         3,
         null,
         25,
         null,
         true,
         "1997-10-20",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "nome",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "idade",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "genero",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ativo",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "data_nascimento",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "ultima_atualizacao",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores nulos tratados por coluna:\nid: 0\nnome: 1\nidade: 1\ngenero: 1\nativo: 0\ndata_nascimento: 1\nultima_atualizacao: 1\nTodos os valores nulos foram tratados.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>nome</th><th>idade</th><th>genero</th><th>ativo</th><th>data_nascimento</th><th>ultima_atualizacao</th></tr></thead><tbody><tr><td>1</td><td>João</td><td>30</td><td>Masculino</td><td>true</td><td>1992-05-15</td><td>2024-04-29T06:55:34.990153Z</td></tr><tr><td>2</td><td>Maria</td><td>0</td><td>Feminino</td><td>false</td><td>1900-01-01</td><td>2024-04-29T06:55:34.990156Z</td></tr><tr><td>3</td><td>Não informado</td><td>25</td><td>Não informado</td><td>true</td><td>1997-10-20</td><td>1900-01-01T00:00:00Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "João",
         30,
         "Masculino",
         true,
         "1992-05-15",
         "2024-04-29T06:55:34.990153Z"
        ],
        [
         2,
         "Maria",
         0,
         "Feminino",
         false,
         "1900-01-01",
         "2024-04-29T06:55:34.990156Z"
        ],
        [
         3,
         "Não informado",
         25,
         "Não informado",
         true,
         "1997-10-20",
         "1900-01-01T00:00:00Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "nome",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "idade",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "genero",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ativo",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "data_nascimento",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "ultima_atualizacao",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define o esquema do DataFrame de exemplo\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"nome\", StringType(), True),\n",
    "    StructField(\"idade\", IntegerType(), True),\n",
    "    StructField(\"genero\", StringType(), True),\n",
    "    StructField(\"ativo\", BooleanType(), True),\n",
    "    StructField(\"data_nascimento\", DateType(), True),\n",
    "    StructField(\"ultima_atualizacao\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Dados do DataFrame de exemplo\n",
    "dados = [\n",
    "    (1, \"João\", 30, \"Masculino\", True, datetime.strptime(\"1992-05-15\", \"%Y-%m-%d\").date(), datetime.now()),\n",
    "    (2, \"Maria\", None, \"Feminino\", False, None, datetime.now()),\n",
    "    (3, None, 25, None, True, datetime.strptime(\"1997-10-20\", \"%Y-%m-%d\").date(), None)\n",
    "]\n",
    "\n",
    "\n",
    "# Cria o DataFrame\n",
    "df_test_null = spark.createDataFrame(dados, schema)\n",
    "\n",
    "# Exibe o DataFrame\n",
    "display(df_test_null)\n",
    "\n",
    "# Chamar a função para tratamento de valores nulos no DataFrame\n",
    "df_trat_null = verificar_e_tratar_valores_nulos(df_test_null)\n",
    "\n",
    "display(df_trat_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94948d90-e0a8-41a0-bcba-84770c3d4ed7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Função para criar arquivo Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6458349-663f-4c26-8d63-8221eb949a63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def criar_arquivo_delta(parquet_file :str, delta_path :str, modo=\"overwrite\") -> None:\n",
    "    \"\"\"\n",
    "    Cria uma tabela Delta a partir de um arquivo Parquet no DBFS do Databricks.\n",
    "\n",
    "    Parâmetros:\n",
    "    arquivo_parquet (str): O caminho do arquivo Parquet origem.\n",
    "    delta_path (str): O caminho onde a tabela Delta será criada.\n",
    "    modo (str, opcional): O modo de escrita. Pode ser \"overwrite\" (substitui), \"append\" (anexa) ou \"ignore\" (ignora). O padrão é \"overwrite\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ler o arquivo Parquet\n",
    "        df = spark.read.parquet(parquet_file)\n",
    "\n",
    "        # Escrever o DataFrame como uma tabela Delta\n",
    "        df.write.format(\"delta\").mode(modo).save(delta_path)\n",
    "        print(\"Arquivo Delta Lake criado com sucesso em:\", delta_path)\n",
    "    except Exception as e:\n",
    "        print(\"Erro ao criar arquivo Delta Lake:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "632226dd-9750-45d0-b9c5-c46c37dcd5d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Aplica etapa de Tratamento de Dados (Valores Nulos e Duplicados)\n",
    "\n",
    "**Premissa**: Realize transformações necessárias, como tratamento de valores nulos, conversões de tipos, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dec8bd47-d60e-4948-b7c1-e94627f05e05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leitura do arquivo Parquet bem-sucedida.\n"
     ]
    }
   ],
   "source": [
    "# Parâmetros de leitura de arquivo\n",
    "path = dbutils.widgets.get('p_path')    #\"dbfs:/FileStore/datum/\"\n",
    "layer = dbutils.widgets.get('p_layer')   #\"bronze/olistbr-brazilian-ecommerce\"\n",
    "file_name_bronze = dbutils.widgets.get('p_file_name_bronze')   #\"olist_customers_dataset\"\n",
    "\n",
    "# Construindo o caminho completo para o arquivo Parquet\n",
    "file_path_bronze = f\"{path}/{layer}/{file_name_bronze}\"\n",
    "\n",
    "# Chamando a função para ler o arquivo Parquet\n",
    "df = ler_parquet(file_path_bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c1a81db-f4fb-4227-82a9-6cfcfac18d6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NÃO existem linhas duplicadas\nValores nulos tratados por coluna:\ncustomer_id: 0\ncustomer_unique_id: 0\ncustomer_zip_code_prefix: 0\ncustomer_city: 0\ncustomer_state: 0\nrow_ingestion_timestamp: 0\nTodos os valores nulos foram tratados.\n"
     ]
    }
   ],
   "source": [
    "# Chamar a função para verificar valores duplicados no DataFrame\n",
    "df_tratado = verificar_e_remover_duplicatas(df)\n",
    "\n",
    "# Chamar a função para verificar valores nulos no DataFrame\n",
    "df_tratado = verificar_e_tratar_valores_nulos(df_tratado)\n",
    "\n",
    "# display(df_tratado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "168ec78d-5c11-4614-919c-8071337a3370",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Opção de grava os dados em parquet ou Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f24993b-1b01-4e44-b9c3-18f64ce21295",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados salvos com sucesso no formato Parquet.\n"
     ]
    }
   ],
   "source": [
    "# Parâmetros de gravação de arquivo\n",
    "file_name_silver = dbutils.widgets.get('p_file_name_silver')    #'customers_silver'\n",
    "destination_path = f'{path}/silver/olistbr-brazilian-ecommerce/{file_name_silver}'\n",
    "mode_write_silver = dbutils.widgets.get('p_mode_write_silver')   #'overwrite'\n",
    "\n",
    "# Salva arquivo em formato parquet, na camada bronze\n",
    "gravar_parquet(df_tratado, destination_path=destination_path, mode_write_bronze=mode_write_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17354748-4eb8-4121-8905-1bc3e56fcc00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parâmetros para salvar arquivo delta\n",
    "table_name = file_name_silver\n",
    "destination_path_delta = f'dbfs:/FileStore/datum/tables/{table_name}'\n",
    "modo_write_delta = dbutils.widgets.get('p_modo_write_delta')   #\"overwrite\"\n",
    "\n",
    "# Chama função para cria arquivo Delta Lake\n",
    "# criar_arquivo_delta(parquet_file=destination_path, delta_path=destination_path_delta, modo=modo_write_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd5c8418-f74e-4119-85fd-f6aa38d87683",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Cria tabela delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0825d896-056e-4acc-b76e-d4f9827bc72e",
     "showTitle": true,
     "title": "Função para criar tabela"
    }
   },
   "outputs": [],
   "source": [
    "def save_dataframe_as_table(df :DataFrame, table_name :str) -> str:\n",
    "    \"\"\"\n",
    "    Cria (se necessário) e usa um banco de dados, exclui uma tabela existente,\n",
    "    e salva um DataFrame como uma nova tabela no Apache Spark.\n",
    "\n",
    "    Parâmetros:\n",
    "        df_tratado (DataFrame): O DataFrame que será salvo como tabela.\n",
    "        table_name (str): O nome da tabela a ser criada/sobreescrita.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Cria o banco de dados 'olist' se ele não existir\n",
    "        spark.sql('CREATE DATABASE IF NOT EXISTS olist')\n",
    "        \n",
    "        # Seleciona o banco de dados 'olist' para uso\n",
    "        spark.sql('USE olist')\n",
    "        \n",
    "        # Exclui a tabela se ela já existir\n",
    "        spark.sql(f'DROP TABLE IF EXISTS {table_name}')\n",
    "        \n",
    "        # Salva o DataFrame como uma tabela, sobrescrevendo se necessário\n",
    "        df.write.mode('overwrite').saveAsTable(f'{table_name}')\n",
    "\n",
    "        return f'Tabela {table_name} criada/atualizada com sucesso no banco de dados olist.'\n",
    "\n",
    "    except Exception as e:\n",
    "        return f'Ocorreu um erro ao processar a tabela {table_name}: {e}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "377db1a6-bcbc-4e30-834b-6f67e5324af3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Tabela customers criada/atualizada com sucesso no banco de dados olist.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chama função para cria tabela\n",
    "save_dataframe_as_table(df_tratado, table_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 508162604964256,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BronzeToSilver",
   "widgets": {
    "p_file_name_bronze": {
     "currentValue": "olist_customers_dataset",
     "nuid": "89ef6575-3423-49aa-90bc-fec6cf94985f",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "p_file_name_bronze",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "p_file_name_silver": {
     "currentValue": "customers",
     "nuid": "2a594336-b3d0-4056-a51e-9c0983f91071",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "p_file_name_silver",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "p_layer": {
     "currentValue": "bronze/olistbr-brazilian-ecommerce",
     "nuid": "61076ed7-2b16-44d1-876a-5d743c0bed07",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "p_layer",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "p_mode_write_silver": {
     "currentValue": "overwrite",
     "nuid": "1b7970f0-0382-4084-a332-98b66827cc90",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "p_mode_write_silver",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "p_modo_write_delta": {
     "currentValue": "overwrite",
     "nuid": "b53f299d-4001-4780-bd8c-7446e1e517f4",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "p_modo_write_delta",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "p_path": {
     "currentValue": "dbfs:/FileStore/datum/",
     "nuid": "43b4bcf0-2b41-486e-874e-66747ffff014",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "p_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
